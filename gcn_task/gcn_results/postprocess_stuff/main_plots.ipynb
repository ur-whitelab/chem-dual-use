{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import cairosvg\n",
    "#import dataframe_image as dfi\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skunk\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "sys.path.insert(1,'../../')\n",
    "from dglgcn import compute_threshold_from_split\n",
    "\n",
    "sys.path.insert(1, '/path/to/application/app/folder')\n",
    "today_date = datetime.today().date()\n",
    "time_now = datetime.today().ctime()\n",
    "\n",
    "# packages & settings for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve('https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf', 'IBMPlexMono-Regular.ttf')\n",
    "fe = font_manager.FontEntry(\n",
    "    fname='IBMPlexMono-Regular.ttf',\n",
    "    name='plexmono')\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update({'axes.facecolor':'#f5f4e9',\n",
    "            'grid.color' : '#AAAAAA',\n",
    "            'axes.edgecolor':'#333333',\n",
    "            'figure.facecolor':'#FFFFFF',\n",
    "            'axes.grid': False,\n",
    "                     \n",
    "            'axes.prop_cycle':   plt.cycler('color', plt.cm.Dark2.colors),\n",
    "            'font.family': fe.name,\n",
    "            'figure.figsize': (3.5,3.5 / 1.2),\n",
    "            'ytick.left': True,\n",
    "            'xtick.bottom': True   ,\n",
    "            'figure.dpi': 300\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "censor_region = \"above\"\n",
    "censor_splits = [0.1, 0.5, 0.9]\n",
    "run_date = '2024-05-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\",\n",
    "    \"./lipophilicity.csv\",\n",
    ")\n",
    "lipodata = pd.read_csv(\"./lipophilicity.csv\")\n",
    "lipodata = list(zip(lipodata.smiles,lipodata.exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cdbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_x_noise_levels(censor_intervals):\n",
    "    '''\n",
    "    Convert similarity ranges to feature noise levels\n",
    "    Low similarity implies high noise level, so we \"inverse\" the numbers by subtracting them from 1\n",
    "    \n",
    "    Arg: censor_intervals is a list of strings with similarity scores\n",
    "    \n",
    "    Returns a list of noise levels (floats)\n",
    "    '''\n",
    "    x_noise_levels = []\n",
    "    for c in censor_intervals:\n",
    "        score1, score2 = map(float, c.split('-')) # convert string to scores\n",
    "        midpoint = (score1 + score2) / 2\n",
    "        x_level = 1 - midpoint \n",
    "        x_noise_levels.append(round(x_level, 3))\n",
    "    return x_noise_levels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import AnnotationBbox\n",
    "import torch\n",
    "\n",
    "sys.path.insert(1,'../')\n",
    "from dglgcn import compute_threshold_from_split\n",
    "\n",
    "def get_file_path(censor_type, censor_split, censor_region, censor_interval, results_dir=None):\n",
    "    # get files that store parity plot data\n",
    "    task = f'{censor_type}_results_split{censor_split}_{censor_region}'\n",
    "    if results_dir is None:\n",
    "        dir_name = f'../all_results/gcn_{task}'\n",
    "    else: \n",
    "        dir_name = results_dir + f'../all_results/gcn_{task}'\n",
    "        \n",
    "    file_name = f'{dir_name}/parityplotdata_trial0_{censor_type}_{censor_interval}.json'\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def make_parity_plot(censor_type, censor_split, censor_region, censor_interval, results_dir=None):\n",
    "    # helper function to make parity plots for inset figures\n",
    "    json_path = get_file_path(censor_type, censor_split, censor_region, censor_interval, results_dir=results_dir)\n",
    "    with open(json_path, 'r') as f:\n",
    "        rmse, lower_rmse, upper_rmse, corr, lower_corr, upper_corr, ytest, yhat = json.load(f)\n",
    "        \n",
    "    labels = [label for _, label in lipodata]\n",
    "    threshold = compute_threshold_from_split(labels, censor_split, censor_region)\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(1,1))\n",
    "    ytest_t, yhat_t = torch.tensor(ytest), torch.tensor(yhat)\n",
    "    upper_ytest = ytest_t[ytest_t >= threshold]\n",
    "    lower_ytest = ytest_t[ytest_t < threshold]\n",
    "    upper_yhat = yhat_t[ytest_t >= threshold]\n",
    "    lower_yhat = yhat_t[ytest_t < threshold]\n",
    "    if censor_region == 'above':\n",
    "        lower_color = 'C0'\n",
    "        upper_color = 'C1'\n",
    "\n",
    "    else:\n",
    "        lower_color = 'C1'\n",
    "        upper_color = 'C0'\n",
    "    ax1.scatter(upper_ytest, upper_yhat, s=1, c=upper_color)\n",
    "    ax1.scatter(lower_ytest, lower_yhat, s=1, c=lower_color)\n",
    "    #ax1.plot([threshold, threshold], [-2, 5], '--', linewidth=0.5, c='C2')\n",
    "    ax1.plot([-2, 5], [-2, 5], c='black', linewidth=0.5)\n",
    "    ax1.set_xlim(-2,5)\n",
    "    ax1.set_ylim(-2,5)\n",
    "\n",
    "    # remove ticks and tick labels to look simple\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    # save to svg file\n",
    "    filename, _ = os.path.splitext(json_path) # remove .json extension\n",
    "    svg_filename = f'{filename}_yt{threshold}.svg'\n",
    "    fig1.patch.set_alpha(0.0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(svg_filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return svg_filename\n",
    "\n",
    "def inset_fig_placeholder(i, ax, xy, censor_type, no_noise=True):\n",
    "    '''\n",
    "    Placing empty annotation boxes for 2nd inset figure in each subplot\n",
    "    Can be before, during, or after 'process_and_plot'\n",
    "    \n",
    "    Steps\n",
    "    1. plot inset figures & save using savefig('filename.svg') -- done\n",
    "    2. make skunk annotation boxes\n",
    "    3. use skunk.insert to insert figures --> last, after legend is added\n",
    "    '''\n",
    "    \n",
    "    # place inset figure holder\n",
    "    if no_noise:\n",
    "        name_box = f'{censor_type}_no-noise{i}'\n",
    "        connectionstyle=\"arc3,rad=-0.2\"\n",
    "        #xybox = (0.03, 0.97) # left top corner\n",
    "        xybox = (0.01,0.97)\n",
    "    else:\n",
    "        name_box = f'{censor_type}_max-noise{i}'\n",
    "        connectionstyle=\"arc3,rad=0.2\"\n",
    "        #xybox = (0.73, 0.97) # right top corner\n",
    "        xybox = (0.76, 0.97)\n",
    "    #box = skunk.Box(75,75, name_box)\n",
    "    box = skunk.Box(45, 45, name_box)\n",
    "    ab = AnnotationBbox(box, xy, # where it points\n",
    "                        xybox=xybox, # where the box is located\n",
    "                        xycoords='data',\n",
    "                        boxcoords=(\"axes fraction\", \"axes fraction\"),\n",
    "                        box_alignment=(0,1),\n",
    "                        arrowprops=dict(arrowstyle='->,head_length=0.4,head_width=0.2',\n",
    "                                        connectionstyle=connectionstyle, \n",
    "                                        fc=\"w\",))\n",
    "    ax.add_artist(ab)\n",
    "    return ax\n",
    "\n",
    "def insert_skunk_figs(censor_types, censor_splits, censor_region, censor_intervals, results_dir=None):\n",
    "    if isinstance(censor_types, str):\n",
    "        censor_types = [censor_types]\n",
    "        censor_intervals = [censor_intervals]\n",
    "    \n",
    "    skunk_dict={}\n",
    "    for j, ctype in enumerate(censor_types):\n",
    "        intervals = censor_intervals[j]\n",
    "        for i, split in enumerate(censor_splits):\n",
    "            # fig for point 0\n",
    "            svg_filename1 = make_parity_plot(ctype, split, censor_region, intervals[0], results_dir=results_dir)\n",
    "            skunk_dict[f'{ctype}_no-noise{i}'] = svg_filename1\n",
    "\n",
    "            # fig for last point\n",
    "            svg_filename2 = make_parity_plot(ctype, split, censor_region, intervals[-1], results_dir=results_dir)\n",
    "            skunk_dict[f'{ctype}_max-noise{i}'] = svg_filename2\n",
    "    svg = skunk.insert(skunk_dict)\n",
    "    return svg\n",
    "\n",
    "def make_xnoise_mapping_table(censor_intervals): #filetype='png'):\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        # hide axes\n",
    "        fig1.patch.set_visible(False)\n",
    "        ax1.axis('off')\n",
    "        ax1.axis('tight')\n",
    "        print('censor_intervals', censor_intervals)\n",
    "        df1 = pd.DataFrame({\n",
    "            'X Noise Level': calculate_x_noise_levels(censor_intervals),\n",
    "            'Similarity Score': censor_intervals,\n",
    "        })\n",
    "        filename = 'xnoise_gcn_mapping_table'\n",
    "\n",
    "        ax1.table(\n",
    "            cellText=df1.values, \n",
    "            colLabels=df1.columns, \n",
    "            loc='center',\n",
    "            colColours =[\"lightblue\"] * 2, # this doesn't work? \n",
    "        )\n",
    "\n",
    "        fig1.tight_layout()\n",
    "        #filename = f'{filename}.{filetype}'\n",
    "        fig1.savefig(f'{filename}.svg', bbox_inches='tight')\n",
    "        fig1.savefig(f'{filename}.png', bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        print(f'Table of X noise levels saved to {filename}')\n",
    "\n",
    "def process_and_plot(i, censor_split, ax, censor_type, run_date, output_dir=None, metric='corr', ending=\"\", hasNaN=False, NaNlist=None, title=True):\n",
    "    task = f'{censor_type}_results_split{censor_split}_{censor_region}{ending}'\n",
    "    if output_dir is None:\n",
    "        dir_name = f'../all_results/gcn_{task}'\n",
    "    else: \n",
    "        dir_name = output_dir + f'all_results/gcn_{task}'\n",
    "    labels = [label for _, label in lipodata]\n",
    "    threshold = compute_threshold_from_split(labels, censor_split, censor_region)\n",
    "    print(f'For censor split {censor_split}, Threshold =',threshold)\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(f'{censor_split * 100:.0f}% sensitive data', fontsize=12)\n",
    "    \n",
    "    if hasNaN and censor_split == 0.9:\n",
    "        # special case for omitting 90% sensitive data --> often gets NaN for correlation values\n",
    "        file_path = f'{dir_name}/dataframe_{run_date}_revised.json'\n",
    "        #ax.set_title(f'{censor_split * 100:.0f}% sensitive data$^*$') # add asterisk\n",
    "    else:\n",
    "        file_path = f'{dir_name}/dataframe_{run_date}.json'\n",
    "        \n",
    "    \n",
    "    # load json file and plot results\n",
    "    df = pd.read_json(file_path)\n",
    "    censor_intervals = df.iloc[:, 0].to_list()\n",
    "    mean_above = df[f'upper {metric}'].to_list()\n",
    "    std_above = df[f'upper {metric} std']\n",
    "    mean_below = df[f'lower {metric}'].to_list()\n",
    "    std_below = df[f'lower {metric} std']\n",
    "    \n",
    "    if censor_type == 'xnoise':\n",
    "        sim_scores = censor_intervals # save the list just in case\n",
    "        censor_intervals = calculate_x_noise_levels(censor_intervals)\n",
    "\n",
    "    # Plot means with fill_between for standard deviation\n",
    "    ax.plot(censor_intervals, mean_above, marker='x', color='C1', label='Sensitive Labels')\n",
    "    ax.fill_between(censor_intervals, np.subtract(mean_above, std_above), np.add(mean_above, std_above), color='C1', alpha=0.2)\n",
    "\n",
    "    ax.plot(censor_intervals, mean_below, marker='^', color='C0', label='Non-sensitive Labels')\n",
    "    ax.fill_between(censor_intervals, np.subtract(mean_below, std_below), np.add(mean_below, std_below), color='C0', alpha=0.2)\n",
    "     \n",
    "    # Placeholders for inset figures\n",
    "    no_noise_xcoord = censor_intervals[0]\n",
    "    max_noise_xcoord = censor_intervals[-1]\n",
    "#     if censor_type == 'xnoise':\n",
    "#         no_noise_xcoord = 0\n",
    "#         max_noise_xcoord = len(censor_intervals)-1\n",
    "#     else:\n",
    "#         no_noise_xcoord = censor_intervals[0]\n",
    "#         max_noise_xcoord = censor_intervals[-1]\n",
    "        \n",
    "    # inset fig for no-noise parity plot, two arrows pointing at two curve regions\n",
    "    xy1a = (no_noise_xcoord, mean_above[0])\n",
    "    xy1b = (no_noise_xcoord, mean_below[0])\n",
    "    inset_fig_placeholder(i, ax, xy1a, censor_type, no_noise=True)\n",
    "    inset_fig_placeholder(i, ax, xy1b, censor_type, no_noise=True) # for 2nd arrow\n",
    "    \n",
    "    # inset fig for max-noise parity plot\n",
    "    xy2a = (max_noise_xcoord, mean_above[-1])\n",
    "    xy2b = (max_noise_xcoord, mean_below[-1])\n",
    "    inset_fig_placeholder(i, ax, xy2a, censor_type, no_noise=False)\n",
    "    inset_fig_placeholder(i, ax, xy2b, censor_type, no_noise=False)\n",
    "\n",
    "    # finalize subplots\n",
    "#     if censor_type == 'xnoise':\n",
    "#         ax.set_xticks(range(len(censor_intervals)))\n",
    "#         ax.set_xticklabels(calculate_x_noise_levels(censor_intervals)) # replace (range(len(censor_intervals)))\n",
    "#         ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))\n",
    "        \n",
    "    ax.autoscale(enable=True, axis='x', tight=True)\n",
    "    if metric == 'corr':\n",
    "        ax.set_ylim(-0.2, 1.4)\n",
    "    else:\n",
    "        ax.set_ylim(0, 4.0)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    if NaNlist is not None and censor_split == 0.9:  # special case\n",
    "        for i in NaNlist:\n",
    "            ax.annotate('*', (censor_intervals[i], mean_above[i]), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=15)\n",
    "            ax.annotate('*', (censor_intervals[i], mean_below[i]), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=15)   \n",
    "    \n",
    "    if censor_type == 'xnoise':\n",
    "        ax.set_xlim(0,1.0)\n",
    "        return sim_scores\n",
    "    return censor_intervals\n",
    "    \n",
    "def plot_all_splits(censor_type, metric='corr', ending='', hasNaN=False, NaNlist=None):\n",
    "    if metric not in ['rmse', 'corr']:\n",
    "        raise ValueError(\"Invalid metric specified. Choose 'rmse' or 'corr'.\")    \n",
    "    elif metric == 'rmse':\n",
    "        ytitle = 'Root Mean Square Error'\n",
    "    else:\n",
    "        ytitle = 'Spearman Correlation'\n",
    "    if censor_type == 'omit':\n",
    "        xtitle = '% sensitive data omitted from training data'\n",
    "    elif censor_type == 'xnoise':\n",
    "        xtitle = 'Feature noise level applied to sensitive data in training data'\n",
    "    elif censor_type == 'ynoise':\n",
    "        xtitle = 'Label noise level applied to sensitive data in training data'\n",
    "    else:\n",
    "        raise ValueError('censor_type must be either \"omit\", \"xnoise\", or \"ynoise\"')\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    axs[0].set_ylabel(ytitle, fontsize=16)\n",
    "    axs[1].set_xlabel(xtitle, labelpad=10, fontsize=16)\n",
    "    \n",
    "    for i, split in enumerate(censor_splits):\n",
    "        censor_intervals = process_and_plot(i, split, axs[i], censor_type, run_date, metric=metric, ending=ending, hasNaN=hasNaN, NaNlist=NaNlist)\n",
    "        \n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], marker='x', color='C1', lw=2, label='Sensitive Labels'),\n",
    "        plt.Line2D([0], [0], marker='^', color='C0', lw=2, label='Non-sensitive Labels')\n",
    "    ]\n",
    "    fig.legend(handles=legend_handles, loc='center', bbox_to_anchor=(0.92, 0.5))\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    \n",
    "    svg = insert_skunk_figs(censor_type, censor_splits, censor_region, censor_intervals)\n",
    "    plt.close()\n",
    "    if censor_type =='xnoise':\n",
    "        make_xnoise_mapping_table(censor_intervals)\n",
    "    return svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine figures of all GCN results\n",
    "\n",
    "# todo: update corresponding noise levels for feature noise\n",
    "def plot_everything(metric='corr'):\n",
    "    if metric not in ['rmse', 'corr']:\n",
    "        raise ValueError(\"Invalid metric specified. Choose 'rmse' or 'corr'.\")\n",
    "    elif metric == 'rmse':\n",
    "        ytitle = 'Root Mean Square Error'\n",
    "    else:\n",
    "        ytitle = 'Spearman Correlation'\n",
    "    \n",
    "    combined_fig, axs = plt.subplots(3, 3, figsize=(10,10), sharey=True) # 3 splits for each of 3 censor types\n",
    "    \n",
    "    censor_intervals = []\n",
    "    \n",
    "    # plot omission results\n",
    "    run_date = '2024-05-11'\n",
    "    #axs[0,1].set_title('GCN Accuracy with Omission') #, fontsize=18)\n",
    "    axs[0,0].set_ylabel(ytitle, fontsize=12)\n",
    "    axs[0,1].set_xlabel('Percentage of Sensitive Data Omitted from Training Data\\n ', fontsize=16) #labelpad=10) #, fontsize=16)\n",
    "    for i, split in enumerate(censor_splits):\n",
    "        omit_intervals = process_and_plot(i, split, axs[0, i], 'omit', run_date, metric=metric, ending=\"_150epochs\", hasNaN=True, NaNlist=[2,8])\n",
    "    censor_intervals.append(omit_intervals)\n",
    "\n",
    "    # plot xnoise results\n",
    "    run_date = '2024-05-06'\n",
    "    #axs[1,1].set_title('GCN Accuracy with $X$ Noise') #, fontsize=18)\n",
    "    axs[1,0].set_ylabel(ytitle, fontsize=12)\n",
    "    axs[1,1].set_xlabel(r'Level of Feature Noise ($\\delta X$) Applied to Sensitive Data in Training Data' + '\\n ', fontsize=16) #, labelpad=10) #, fontsize=16)\n",
    "    for i, split in enumerate(censor_splits):\n",
    "        xnoise_intervals = process_and_plot(i, split, axs[1, i], 'xnoise', run_date, metric=metric) #, title=False)\n",
    "    make_xnoise_mapping_table(xnoise_intervals)\n",
    "    censor_intervals.append(xnoise_intervals)\n",
    "    \n",
    "\n",
    "    # plot ynoise results\n",
    "    run_date = '2024-05-11'\n",
    "    #axs[2,1].set_title('GCN Accuracy with $y$ Noise') #, fontsize=18)\n",
    "    axs[2,0].set_ylabel(ytitle, fontsize=12)\n",
    "    axs[2,1].set_xlabel(r'Level of Label Noise ($\\delta y$) Applied to Sensitive Data in Training Data', fontsize=16) #, labelpad=10) #, fontsize=16)\n",
    "    for i, split in enumerate(censor_splits):\n",
    "        ynoise_intervals = process_and_plot(i, split, axs[2, i], 'ynoise', run_date, metric=metric) #, title=False)\n",
    "    censor_intervals.append(ynoise_intervals)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=1.0)\n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], marker='x', color='C1', lw=2, label='Sensitive Labels'),\n",
    "        plt.Line2D([0], [0], marker='^', color='C0', lw=2, label='Non-sensitive Labels')\n",
    "    ]\n",
    "    combined_fig.legend(handles=legend_handles, bbox_to_anchor=(0.97, 0.06)) #, loc='center') #, bbox_to_anchor=(0.92, 0.5))\n",
    "    \n",
    "    combined_fig.tight_layout(rect=[0, 0.05, 1, 1]) #tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    \n",
    "    svg = insert_skunk_figs(['omit', 'xnoise', 'ynoise'], censor_splits, censor_region, censor_intervals)\n",
    "    plt.close()\n",
    "    return svg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svg = plot_everything()\n",
    "skunk.display(svg)\n",
    "\n",
    "with open('paper_figs/gcn_corr_with_all_titles_2024-12-31.svg', 'w') as f:\n",
    "    f.write(svg)\n",
    "\n",
    "cairosvg.svg2png(bytestring=svg, write_to='paper_figs/gcn_corr_with_all_titles_2024-12-31.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4342b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cairosvg.svg2png(bytestring=svg, write_to='paper_figs/gcn_corr_with_all_titles_2024-08-27.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cba5d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svg = plot_all_splits(censor_type='omit', metric='rmse')\n",
    "skunk.display(svg)\n",
    "\n",
    "# with open('replaced.svg', 'w') as f:\n",
    "#     f.write(svg)\n",
    "\n",
    "#cairosvg.svg2png(bytestring=svg, write_to='overview_gcn_omit_rmse.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4fd28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_date = '2024-05-06'\n",
    "svg = plot_all_splits(censor_type='xnoise', metric='rmse')\n",
    "skunk.display(svg)\n",
    "\n",
    "#cairosvg.svg2png(bytestring=svg, write_to='overview_gcn_xnoise_rmse.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dualusage",
   "language": "python",
   "name": "dualusage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
