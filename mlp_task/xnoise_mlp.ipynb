{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd547d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve('https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf', 'IBMPlexMono-Regular.ttf')\n",
    "fe = font_manager.FontEntry(\n",
    "    fname='IBMPlexMono-Regular.ttf',\n",
    "    name='plexmono')\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update({'axes.facecolor':'#f5f4e9',\n",
    "            'grid.color' : '#AAAAAA',\n",
    "            'axes.edgecolor':'#333333',\n",
    "            'figure.facecolor':'#FFFFFF',\n",
    "            'axes.grid': False,\n",
    "            'axes.prop_cycle':   plt.cycler('color', plt.cm.Dark2.colors),\n",
    "            'font.family': fe.name,\n",
    "            'figure.figsize': (3.5,3.5 / 1.2),\n",
    "            'ytick.left': True,\n",
    "            'xtick.bottom': True   ,\n",
    "            'figure.dpi': 300\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707333fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    Din: int = 50 # dim of features\n",
    "    hidden_dim: int = 64\n",
    "    batchsize: int = 32\n",
    "    datasize: int = 6400\n",
    "    split: float = 0.1 # 10/10/80 test val train\n",
    "    epochs: int = 60\n",
    "    lr: float = 0.001\n",
    "    patience: int = 5\n",
    "    min_delta: float = 1e-4 # for early stopping\n",
    "\n",
    "config = Config()\n",
    "seed = 511\n",
    "\n",
    "# censor data settings\n",
    "censor_split = .1 # 10/90 sensitive data/ non-sensitive data\n",
    "censor_region = 'above'\n",
    "\n",
    "dir_name = '../mlp_xnoise_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e510b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_noise_levels = np.linspace(0, 2, int(2/0.1+1)) \n",
    "\n",
    "tasks = []\n",
    "for x_level in x_noise_levels:\n",
    "    task = (f\"xn_level{x_level:0.1f}\", x_level, 0, False, None)\n",
    "    print(task)\n",
    "    tasks.append(task)\n",
    "\n",
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/path/to/application/app/folder')\n",
    "\n",
    "from mlp_fxns import mlptask_wrapper_v2\n",
    "\n",
    "num_trials = 5\n",
    "all_trials_results = []\n",
    "for trial in range(num_trials):\n",
    "    trial_seed = seed + trial\n",
    "    print(f'\\033[46mTrial {trial+1}, Seed: {trial_seed}\\033[0m')\n",
    "    results = mlptask_wrapper_v2(\n",
    "        trial_seed, \n",
    "        tasks, \n",
    "        censor_region, \n",
    "        censor_split,\n",
    "        model_config=config,\n",
    "        verbose=True, \n",
    "        sanitycheckplot=False\n",
    "    )\n",
    "    all_trials_results.append(results)\n",
    "    \n",
    "time_now = datetime.today().ctime()\n",
    "with open(f'{dir_name}/history.json','a') as f:\n",
    "    f.write(f'\\nRun from today: {time_now}')\n",
    "    json.dump([all_trials_results],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results of last trial \n",
    "from plot_fxns import plot_trainingcurves, plot_parityplots\n",
    "\n",
    "plot_trainingcurves(tasks[:4], x_results)\n",
    "plot_parityplots(tasks[:4], x_results, threshold=x_results['censor_threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean & std for all trials\n",
    "aggregated_results ={\n",
    "    'x_noise_level': list(x_noise_levels),\n",
    "    'y_noise_level': 0,\n",
    "    'omit': False,\n",
    "    'overall_error_mean': [],\n",
    "    'overall_error_std': [],\n",
    "    'lower_error_mean': [],\n",
    "    'lower_error_std': [],\n",
    "    'upper_error_mean': [],\n",
    "    'upper_error_std': [],\n",
    "}\n",
    "\n",
    "for x_level in x_noise_levels:\n",
    "    overall_errors = []\n",
    "    lower_errors = []\n",
    "    upper_errors = []\n",
    "    for result in all_trials_results:\n",
    "        overall_errors.append(result['overall_error'][f'xn_level{x_level:0.1f}'])\n",
    "        lower_errors.append(result['lower_error'][f'xn_level{x_level:0.1f}'])\n",
    "        upper_errors.append(result['upper_error'][f'xn_level{x_level:0.1f}'])\n",
    "        \n",
    "    aggregated_results['overall_error_mean'].append(np.mean(overall_errors))\n",
    "    aggregated_results['overall_error_std'].append(np.std(overall_errors))\n",
    "    aggregated_results['lower_error_mean'].append(np.mean(lower_errors))\n",
    "    aggregated_results['lower_error_std'].append(np.std(lower_errors))\n",
    "    aggregated_results['upper_error_mean'].append(np.mean(upper_errors))\n",
    "    aggregated_results['upper_error_std'].append(np.std(upper_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b772425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check censor_threshold, calculated based on chosen split of sensitive/non-sensitive data\n",
    "# generated data was different every time the seed changes\n",
    "\n",
    "censor_thresholds = [result['censor_threshold'] for result in all_trials_results]\n",
    "print(censor_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from plot_fxns import create_dataframe\n",
    "today_date = datetime.today().date()\n",
    "\n",
    "df_x = create_dataframe(aggregated_results)\n",
    "df_x.to_json(f'{dir_name}/summary_{today_date}.json')\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.scatter(df_x['s=0 RMSE'], df_x['s=1 RMSE'])\n",
    "plt.title('Test Errors')\n",
    "plt.xlabel('RMSE for non-sensitive region')\n",
    "plt.ylabel('RMSE for sensitive region')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "# TODO: Add omission method to compare. Make the lines dashed. Add 'omission' text nearby (or include it in legends)\n",
    "# omit_s0 = main_results['lower_error']['omission'] \n",
    "# omit_s1 = main_results['upper_error']['omission']\n",
    "# plt.axhline(y=omit_s0, xmin=0, xmax=2, alpha=0.7)\n",
    "# plt.axhline(y=omit_s1, xmin=0, xmax=2, c='C1', alpha=0.7)\n",
    "\n",
    "# plt.plot(df_x['x noise'], df_x['s=0 RMSE'], marker='o', label='Non-sensitive region')\n",
    "# plt.plot(df_x['x noise'], df_x['s=1 RMSE'], marker='o', label='Sensitive region')\n",
    "plt.errorbar(df['x noise'], df['s=1 RMSE'], yerr=df['s=1 RMSE std'], marker='o', label='Sensitive data', capsize=5)\n",
    "plt.errorbar(df['x noise'], df['s=0 RMSE'], yerr=df['s=0 RMSE std'], marker='o', label='Non-sensitive data', capsize=5)\n",
    "plt.title('Test Errors')\n",
    "plt.xlabel('x noise level')\n",
    "plt.ylabel('RMSE')\n",
    "#plt.ylim(0,1.8)\n",
    "plt.xlim(0,2.0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{dir_name}/mlp_testerror_xnoise_{today_date}.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['x noise'], df['s=1 RMSE'], label='Sensitive data', marker='o', markersize=2)\n",
    "plt.fill_between(df['x noise'], df['s=1 RMSE'] - df['s=1 RMSE std'], df['s=1 RMSE'] + df['s=1 RMSE std'], alpha=0.2)\n",
    "\n",
    "plt.plot(df['x noise'], df['s=0 RMSE'], label='Non-sensitive data', marker='o', markersize=2)\n",
    "plt.fill_between(df['x noise'], df['s=0 RMSE'] - df['s=0 RMSE std'], df['s=0 RMSE'] + df['s=0 RMSE std'], alpha=0.2)\n",
    "\n",
    "plt.title('Test Errors')\n",
    "plt.xlabel('x noise level')\n",
    "plt.ylabel('RMSE')\n",
    "#plt.ylim(0,1.8)\n",
    "plt.xlim(0,2.0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{dir_name}/mlp_testerror_xnoise_{today_date}_v2.png',dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dualuse",
   "language": "python",
   "name": "dualuse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
