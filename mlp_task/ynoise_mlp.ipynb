{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab18739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '/path/to/application/app/folder')\n",
    "time_now = datetime.today().ctime()\n",
    "today_date = datetime.today().date()\n",
    "\n",
    "# packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve('https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf', 'IBMPlexMono-Regular.ttf')\n",
    "fe = font_manager.FontEntry(\n",
    "    fname='IBMPlexMono-Regular.ttf',\n",
    "    name='plexmono')\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update({'axes.facecolor':'#f5f4e9',\n",
    "            'grid.color' : '#AAAAAA',\n",
    "            'axes.edgecolor':'#333333',\n",
    "            'figure.facecolor':'#FFFFFF',\n",
    "            'axes.grid': False,\n",
    "            'axes.prop_cycle':   plt.cycler('color', plt.cm.Dark2.colors),\n",
    "            'font.family': fe.name,\n",
    "            'figure.figsize': (3.5,3.5 / 1.2),\n",
    "            'ytick.left': True,\n",
    "            'xtick.bottom': True   ,\n",
    "            'figure.dpi': 300\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b0ede",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# select either threshold OR split to divide sensitive vs non-sensitive\n",
    "censor_threshold = None\n",
    "censor_region = 'above'\n",
    "censor_split = 0.1             # 10% sensitive data, 90% non-sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc037f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    Din: int = 50                 # dim of features\n",
    "    hidden_dim: int = 64\n",
    "    batchsize: int = 32\n",
    "    datasize: int = 6400\n",
    "    split: float = 0.1            # 10/10/80 test val train\n",
    "    epochs: int = 60\n",
    "    lr: float = 0.001\n",
    "    patience: int = 5\n",
    "    min_delta: float = 1e-4       # for early stopping\n",
    "\n",
    "config = Config()\n",
    "seed = 511\n",
    "dir_name = f'OUTPUTS/all_results/mlp_ynoise_results_split{censor_split}_{censor_region}'\n",
    "fig_dir_name = f'OUTPUTS/figures/'\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "os.makedirs(fig_dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e059f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noise_levels = np.linspace(0, 10, int(2/0.2+1))\n",
    "y_tasks = []\n",
    "for y_level in y_noise_levels:\n",
    "    task = (f\"y noise level {y_level}\", 0, y_level, False, None)\n",
    "    print(task)\n",
    "    y_tasks.append(task)\n",
    "\n",
    "len(y_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_fxns import mlptask_wrapper\n",
    "\n",
    "num_trials = 5\n",
    "all_trials_results = []\n",
    "for trial in range(num_trials):\n",
    "    trial_seed = seed + trial\n",
    "    print(f'\\033[46mTrial {trial+1}, Seed: {trial_seed}\\033[0m')\n",
    "    results = mlptask_wrapper(\n",
    "        trial_seed, \n",
    "        y_tasks, \n",
    "        censor_region, \n",
    "        censor_split,\n",
    "        model_config=config,\n",
    "        verbose=True, \n",
    "        sanitycheckplot=False\n",
    "    )\n",
    "    all_trials_results.append(results)\n",
    "    \n",
    "\n",
    "try: \n",
    "    with open(f'{dir_name}/history.json','a') as f:\n",
    "        f.write(f'\\nRun from today: {time_now}\\n')\n",
    "        json.dump(all_trials_results, f, indent=4)\n",
    "except Exception as e:\n",
    "    template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "    message = template.format(type(e).__name__, e.args)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768bc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plot_fxns import plot_trainingcurves, plot_parityplots\n",
    "\n",
    "# check the results of last trial \n",
    "plot_trainingcurves(y_tasks[:4], results)\n",
    "plot_parityplots(y_tasks[:4], results, threshold=results['censor_threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean & std for all trials\n",
    "aggregated_results ={\n",
    "    'x_noise_level': 0,\n",
    "    'y_noise_level': list(y_noise_levels),\n",
    "    'omit': False,\n",
    "    'overall_error_mean': [],\n",
    "    'overall_error_std': [],\n",
    "    'lower_error_mean': [],\n",
    "    'lower_error_std': [],\n",
    "    'upper_error_mean': [],\n",
    "    'upper_error_std': [],\n",
    "}\n",
    "\n",
    "for y_level in y_noise_levels:\n",
    "    task_name = f\"y noise level {y_level}\"\n",
    "    overall_errors = []\n",
    "    lower_errors = []\n",
    "    upper_errors = []\n",
    "    for result in all_trials_results:\n",
    "        overall_errors.append(result['overall_error'][task_name])\n",
    "        lower_errors.append(result['lower_error'][task_name])\n",
    "        upper_errors.append(result['upper_error'][task_name])\n",
    "        \n",
    "    aggregated_results['overall_error_mean'].append(np.mean(overall_errors))\n",
    "    aggregated_results['overall_error_std'].append(np.std(overall_errors))\n",
    "    aggregated_results['lower_error_mean'].append(np.mean(lower_errors))\n",
    "    aggregated_results['lower_error_std'].append(np.std(lower_errors))\n",
    "    aggregated_results['upper_error_mean'].append(np.mean(upper_errors))\n",
    "    aggregated_results['upper_error_std'].append(np.std(upper_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check censor_threshold\n",
    "# generated data was different every time the seed changes\n",
    "\n",
    "censor_thresholds = [result['censor_threshold'] for result in all_trials_results]\n",
    "print(censor_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64735b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_fxns import create_dataframe\n",
    "\n",
    "df = create_dataframe(aggregated_results)\n",
    "df.to_json(f'{dir_name}/summary_{today_date}.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(f'{dir_name}/summary_{today_date}.json')\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "# TODO: Add omission method to compare. Make the lines dashed. Add 'omission' text nearby (or include it in legends)\n",
    "# omit_s0 = main_results['lower_error']['omission'] \n",
    "# omit_s1 = main_results['upper_error']['omission']\n",
    "# plt.axhline(y=omit_s0, xmin=0, xmax=2, alpha=0.7)\n",
    "# plt.axhline(y=omit_s1, xmin=0, xmax=2, c='C1', alpha=0.7)\n",
    "\n",
    "# plt.plot(df_y['y noise'], df_y['s=0 RMSE'], marker='o', label='Non-sensitive region')\n",
    "# plt.plot(df_y['y noise'], df_y['s=1 RMSE'], marker='o', label='Sensitive region')\n",
    "plt.errorbar(df['y noise'], df['s=1 RMSE'], yerr=df['s=1 RMSE std'], marker='o', label='Sensitive data', capsize=5)\n",
    "plt.errorbar(df['y noise'], df['s=0 RMSE'], yerr=df['s=0 RMSE std'], marker='o', label='Non-sensitive data', capsize=5)\n",
    "plt.title('Test Errors')\n",
    "plt.xlabel('y noise level')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.ylim(0,1.8)\n",
    "#plt.xticks(x_ticks[1::2])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{fig_dir_name}/mlp_testerror_ynoise_split{censor_split}_{censor_region}_{today_date}.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(f'{dir_name}/summary_{today_date}.json')\n",
    "\n",
    "plt.plot(df['y noise'], df['s=1 RMSE'], label='Sensitive data', marker='o', markersize=2)\n",
    "plt.fill_between(df['y noise'], df['s=1 RMSE'] - df['s=1 RMSE std'], df['s=1 RMSE'] + df['s=1 RMSE std'], alpha=0.2)\n",
    "\n",
    "plt.plot(df['y noise'], df['s=0 RMSE'], label='Non-sensitive data', marker='o', markersize=2)\n",
    "plt.fill_between(df['y noise'], df['s=0 RMSE'] - df['s=0 RMSE std'], df['s=0 RMSE'] + df['s=0 RMSE std'], alpha=0.2)\n",
    "\n",
    "plt.title('Test Errors')\n",
    "plt.xlabel('y noise level')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim(0,1.8)\n",
    "plt.xlim(0,10)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{fig_dir_name}/mlp_testerror_ynoise_split{censor_split}_{censor_region}_{today_date}_v2.png',dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "dualusage",
   "language": "python",
   "name": "dualusage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
